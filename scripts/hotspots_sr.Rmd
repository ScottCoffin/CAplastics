---
title: "Exploratory Hotspot Analysis"
author: "Sydney Rilum"
date: "2023-04-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(here)
library(readxl)
library(janitor)
library(tidycensus)   # for census data
#census_api_key("9e2f392f2e528a2aa2f46238ae0a192d45b96ad8", install = TRUE) # only need to run this code once
library(sf)
library(purrr)
library(stringr)
library(tmap)
library(sp)       # for data transformation, CRS projection
library(rgeos)    # for buffering points
```

# Read in Data

## Ocean Conservancy Trash Data

### Code from Corey's explore_oc.R script
```{r}
# load & clean data
oc_raw <- read_excel(here("data", "OceanConservancy_CA.xlsx"), 
                     col_types = c("numeric", "text", "text", "text", "text", 
                                   "text", "date", "text", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric"))
colnames(oc_raw)
str(oc_raw)
head(oc_raw)
```

```{r}
## keep only Coastal Cleanup Day dates, note Sept. 2020 was "coastal cleanup month"
ccd_dates <- c("2016-09-17", "2017-09-16", "2018-09-15", "2019-09-21", "2021-09-19")
ccd_dates <- as.Date(ccd_dates)

oc_use <- oc_raw %>%
  mutate(`Cleanup Date` = as.Date(`Cleanup Date`)) %>%
  dplyr::filter(`Cleanup Date` %in% ccd_dates | 
                  (`Cleanup Date` >= as.Date('2020-09-01') & `Cleanup Date` <= as.Date('2020-09-30')))

levels(as.factor(oc_use$`Cleanup Date`)) ## double check
```

```{r}
## remove explicitly non-plastic items and items that cannot be identified
rm_nonp <- c("Bottle Caps (Metal)", "Beverage Bottles (Glass)", "Beverage Cans", "Paper Bags", "Cups, Plates (Paper)", "Other Plastic/Foam Packaging", "Other Packaging (Clean Swell)", "Other Trash (Clean Swell)", "Personal Hygiene (Clean Swell)", "Foam Pieces", "Glass Pieces", "Plastic Pieces")

oc_use <- oc_use %>%
  select(-any_of(rm_nonp))

## continue cleaning, exploring & filtering dataset
oc_use <- janitor::clean_names(oc_use)
colnames(oc_use)
str(oc_use)
n_distinct(oc_use$cleanup_id) ## should equal nobs in oc_use. Cleanup ID can be used as an index column
```

```{#r}
## create separate metadata table and tidy table, which removes rows with values = 0
oc_meta <- oc_use %>%
  dplyr::select(cleanup_id:number_of_bags)

oc_tidy <- oc_use %>%
  dplyr::select(cleanup_id, zone, cleanup_date, people, miles, cigarette_butts:gloves_masks_ppe) %>%
  pivot_longer(cols = cigarette_butts:gloves_masks_ppe, names_to = "item", values_to = "value") %>%
  drop_na(value)
oc_tidy$year <- as.numeric(format(oc_tidy$cleanup_date, "%Y"))

## double-check: did this remove rows where no specific items were reported as Sarah K warned on Nov 15?
zero_rows <- oc_use %>%
  filter_at(vars(15:52), any_vars(! is.na(.)))
n_distinct(zero_rows$cleanup_id)
n_distinct(oc_tidy$cleanup_id) ## if equal, then yes. 
```

### Sydney's code below
```{r}
# create a new sf dataframe that uses GPS column to make geometry for mapping purposes
oc_sf <- oc_use %>% 
  # split up gps column into lat and long columns
  separate(gps, into = c("latitude", "longitude"), sep = ", ", remove = FALSE, extra = "merge") %>%
  # use lat and long coordinates to create a point geometry column
  st_as_sf(., coords = c("longitude","latitude"))

head(oc_sf) # check geometry type (hidden)
```

```{r}
# set CRS
st_crs(oc_sf) <- 4269 # EPSG for NAD83

# check CRS
st_crs(oc_sf)
```

```{r}
# map cleanup sites
ggplot() +
  geom_sf(data = oc_sf) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Trash Cleanup Sites")
```

## Census Tract Data

Use `tidycensus` package to obtain census track data of California.
```{r}
# view 2020 census data variables
#var <- load_variables(2020, "pl", cache = TRUE)
#view(var)

# view 2021 ACS data variables (NOTE: 2022 ACS data not available yet...)
var_acs <- load_variables(2021, "acs5", cache = TRUE)
view(var_acs)
```

```{r}
# to cache shapefiles for use in future sessions
options(tigris_use_cache = TRUE)

# obtain data and feature geometry from the 2020 census (from tidycensus package)
#ca_census <- get_decennial(geography = "tract",
#                           variables = "H1_001N",
#                           year = 2020,
#                           state = "CA",
#                           geometry = TRUE)

# obtain CA census track data and feature geometry from the 2017-2021 5 year ACS (American Community Survey)
ca_acs <- get_acs(geography = "tract",
                  survey = "acs5",
                  variables = "B01003_001", # total population (estimate and MOE/margin-of-error)
                  year = 2021,
                  state = "CA",
                  geometry = TRUE) # obtain geometry
```

```{r}
# map census tracts
ggplot() +
  geom_sf(data = ca_acs) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{#r}
# map census tract population density
ggplot() +
  geom_sf(data = ca_acs, aes(fill = estimate), color = "black") +
  scale_fill_gradientn(colors = c("lightgray","orange","red")) +
  theme_minimal() +
  labs(fill = "ACS Census Tract Population Estimates",
       x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{r}
# check CRS (Coordinate Reference System) match
st_crs(ca_acs) # NAD83
st_crs(oc_sf)
```


# Spatial Join

```{r}
# map trash cleanup sites over census tract outlines
ggplot() +
  geom_sf(data = ca_acs) +
  geom_sf(data = oc_sf, color = "blue") +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Coastal Cleanup Sites")
```

Conduct a spatial join of ACS Census tracts (polygons) and Trash Cleanup sites (points).
```{r}
# spatial join
oc_join <- ca_acs %>% 
  st_join(oc_sf)
```

Determine the number of cleanup sites (points) within each census tract (polygon).
```{r}
# count number of points per polygon
cleanup_counts <- oc_join %>% 
  count(NAME)
```

Summary Stats:
```{r}
mean(cleanup_counts$n)
median(cleanup_counts$n)
max(cleanup_counts$n)
min(cleanup_counts$n)
```

```{r}
unique(cleanup_counts$n)
```

```{r}
cleanup_counts1 <- cleanup_counts %>% 
  filter(n == "1")

count(cleanup_counts1) # 8,176 of 9,129 census tracts have only 1 cleanup site
```


# Sept. 2020 "Cleanup Month" 

## Buffering Cleanup locations

```{r}
# filter for Sept. 2020 cleanup month data
sept2020 <- oc_sf %>% 
  filter(cleanup_date >= '2020-09-01' & cleanup_date <= '2020-09-30')
```

```{r}
# spatial join
sept2020_join <- ca_acs %>% 
  st_join(sept2020) 
```

```{r}
st_crs(sept2020_join) #NAD83, ESPG 4269
```

```{#r}
# before buffering points, transform data from a geographic CRS (NAD83) to a projected CRS

# projected CRS
#pcrs <- CRS("+init=epsg:2229")
##
pcrs <- CRS("+proj=utm +datum=NAD83 +units=km")

# transform dataset to projected CRS
transform <- spTransform(sept2020_join, pcrs)

#create booths with 1km buffer
transform_buffer1km <- gBuffer(transform, byid = TRUE, width = 1, capStyle="ROUND")


#sept2020_join <- st_crs(2229)
#st_crs(sept2020_join)
```

#?????????????????????????????????????????????work in progress

```{#r}
# Buffer Sept. 2020 Cleanup Sites
distm <- 100 # distance in meters
buffer100m <- gBuffer(sept2020_join, width = 1*distm, byid = TRUE)

# Add data, and write to shapefile
buffer100m <- SpatialPolygonsDataFrame( buffer100m, data = buffer100m@data )
writeOGR( buffer100m, "buffer100m", "buffer100m", driver="ESRI Shapefile" ) 
```

```{#r}
# To get Statscan CRS, see here:
# http://spatialreference.org/ref/epsg/3347/
pc <- spTransform(sept2020_join, CRS("+init=epsg:2229")) 

pc100km <- gBuffer( pc, width=100*distm, byid=TRUE )
# Add data, and write to shapefile
pc100km <- SpatialPolygonsDataFrame( pc100km, data=pc100km@data )
writeOGR( pc100km, "pc100km", "pc100km", driver="ESRI Shapefile" ) 
```



### another try
```{r}
# convert data frame to sf
sept2020sites <- st_as_sf(sept2020, coords = "gps")

# convert sp to sf
#tracts <- st_as_sf(ca_acs)

# add cleanup site coordinate system to tracts
#st_crs(sept2020sites) = st_crs(ca_acs)
```

```{r}
# transform to metric coordinate system and create buffer around each cleanup site by 1 km
sept2020sites_km = st_transform(sept2020sites, "+proj=longlat +datum=NAD83 +units=km") %>% 
  st_buffer(dist = units::set_units(1, km))

#tracts_km = st_transform(tracts, "+proj=longlat +datum=NAD83 +units=km")
```

```{r}
# view buffered sites for LA county only

la_county <- sept2020sites_km %>% 
  filter(zone == "Los Angeles County, CA, USA")

la_tracts <- ca_acs %>% 
  filter(str_detect(NAME, 'Los Angeles'))

ggplot() +
  geom_sf(data = la_county) 
```

```{#r}
# create 1km buffer zone
sept2020sites_buffer = st_buffer(sept2020sites_km, 1)
sites_tracts = st_intersection(sept2020sites_buffer, tracts_km)
```

```{r}
# how many intersections?
dim(la_county)

# compute area of intersection
la_county$area = st_area(la_county)

# count number of times each buffered site overlaps with another buffered site
la_county$pt_count <- lengths(st_intersects(la_county, la_county))
la_county$pt_count


# ..... maybe make some sort of for loop... when point count is greater than 1, select site with earliest 'cleanup_date' and remove all other overlapping sites???
```











--------------

CA Shoreline shapefile (not needed?)
```{#r}
# read in CA shoreline 
ca_shoreline <- read_sf(here("data","CA_Shoreline","cstlne_simple.shp")) %>% 
  clean_names()
```

```{#r}
ggplot() +
  geom_sf(data = ca_shoreline)
```

