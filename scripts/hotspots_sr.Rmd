---
title: "Exploratory Hotspot Analysis"
author: "Sydney Rilum"
date: "2023-04-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(here)
library(readxl)
library(janitor)
library(tidycensus)   # for census data
#census_api_key("9e2f392f2e528a2aa2f46238ae0a192d45b96ad8", install = TRUE) # only need to run this code once
library(sf)
library(purrr)
library(stringr)
library(tmap)
library(sp)       # for data transformation, CRS projection
library(rgeos)    # for buffering points
```

# Read in Data

## Ocean Conservancy Trash Data

### Code from Corey's explore_oc.R script
```{r}
# load & clean data
oc_raw <- read_excel(here("data", "OceanConservancy_CA.xlsx"), 
                     col_types = c("numeric", "text", "text", "text", "text", 
                                   "text", "date", "text", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric",
                                   "numeric", "numeric", "numeric", "numeric", "numeric"))
colnames(oc_raw)
str(oc_raw)
head(oc_raw)
```

```{r}
## keep only Coastal Cleanup Day dates, note Sept. 2020 was "coastal cleanup month"
ccd_dates <- c("2016-09-17", "2017-09-16", "2018-09-15", "2019-09-21", "2021-09-19")
ccd_dates <- as.Date(ccd_dates)

oc_use <- oc_raw %>%
  mutate(`Cleanup Date` = as.Date(`Cleanup Date`)) %>%
  dplyr::filter(`Cleanup Date` %in% ccd_dates | 
                  (`Cleanup Date` >= as.Date('2020-09-01') & `Cleanup Date` <= as.Date('2020-09-30')))

levels(as.factor(oc_use$`Cleanup Date`)) ## double check
```

```{r}
## remove explicitly non-plastic items and items that cannot be identified
rm_nonp <- c("Bottle Caps (Metal)", "Beverage Bottles (Glass)", "Beverage Cans", "Paper Bags", "Cups, Plates (Paper)", "Other Plastic/Foam Packaging", "Other Packaging (Clean Swell)", "Other Trash (Clean Swell)", "Personal Hygiene (Clean Swell)", "Foam Pieces", "Glass Pieces", "Plastic Pieces")

oc_use <- oc_use %>%
  select(-any_of(rm_nonp))

## continue cleaning, exploring & filtering dataset
oc_use <- janitor::clean_names(oc_use)
colnames(oc_use)
str(oc_use)
n_distinct(oc_use$cleanup_id) ## should equal nobs in oc_use. Cleanup ID can be used as an index column
```

```{#r}
## create separate metadata table and tidy table, which removes rows with values = 0
oc_meta <- oc_use %>%
  dplyr::select(cleanup_id:number_of_bags)

oc_tidy <- oc_use %>%
  dplyr::select(cleanup_id, zone, cleanup_date, people, miles, cigarette_butts:gloves_masks_ppe) %>%
  pivot_longer(cols = cigarette_butts:gloves_masks_ppe, names_to = "item", values_to = "value") %>%
  drop_na(value)
oc_tidy$year <- as.numeric(format(oc_tidy$cleanup_date, "%Y"))

## double-check: did this remove rows where no specific items were reported as Sarah K warned on Nov 15?
zero_rows <- oc_use %>%
  filter_at(vars(15:52), any_vars(! is.na(.)))
n_distinct(zero_rows$cleanup_id)
n_distinct(oc_tidy$cleanup_id) ## if equal, then yes. 
```

### Sydney's code below
```{r}
# create a new sf dataframe that uses GPS column to make geometry for mapping purposes
oc_sf <- oc_use %>% 
  # split up gps column into lat and long columns
  separate(gps, into = c("latitude", "longitude"), sep = ", ", remove = FALSE, extra = "merge") %>%
  # use lat and long coordinates to create a point geometry column
  st_as_sf(., coords = c("longitude","latitude"))

head(oc_sf) # check geometry type (hidden)
```

```{r}
# set CRS
st_crs(oc_sf) <- 4269 # EPSG for NAD83

# check CRS
st_crs(oc_sf)
```

```{r}
# map cleanup sites
ggplot() +
  geom_sf(data = oc_sf) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Trash Cleanup Sites")
```

## Census Tract Data

Use `tidycensus` package to obtain census track data of California.
```{r}
# view 2020 census data variables
#var <- load_variables(2020, "pl", cache = TRUE)
#view(var)

# view 2021 ACS data variables (NOTE: 2022 ACS data not available yet...)
var_acs <- load_variables(2021, "acs5", cache = TRUE)
view(var_acs)
```

```{r}
# to cache shapefiles for use in future sessions
options(tigris_use_cache = TRUE)

# obtain data and feature geometry from the 2020 census (from tidycensus package)
#ca_census <- get_decennial(geography = "tract",
#                           variables = "H1_001N",
#                           year = 2020,
#                           state = "CA",
#                           geometry = TRUE)

# obtain CA census track data and feature geometry from the 2017-2021 5 year ACS (American Community Survey)
ca_acs <- get_acs(geography = "tract",
                  survey = "acs5",
                  variables = "B01003_001", # total population (estimate and MOE/margin-of-error)
                  year = 2021,
                  state = "CA",
                  geometry = TRUE) # obtain geometry
```

```{r}
# map census tracts
ggplot() +
  geom_sf(data = ca_acs) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{#r}
# map census tract population density
ggplot() +
  geom_sf(data = ca_acs, aes(fill = estimate), color = "black") +
  scale_fill_gradientn(colors = c("lightgray","orange","red")) +
  theme_minimal() +
  labs(fill = "ACS Census Tract Population Estimates",
       x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{r}
# check CRS (Coordinate Reference System) match
st_crs(ca_acs) # NAD83
st_crs(oc_sf)
```


# Spatial Join

```{r}
# map trash cleanup sites over census tract outlines
ggplot() +
  geom_sf(data = ca_acs) +
  geom_sf(data = oc_sf, color = "blue") +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Coastal Cleanup Sites")
```

Conduct a spatial join of ACS Census tracts (polygons) and Trash Cleanup sites (points).
```{r}
# spatial join
oc_join <- ca_acs %>% 
  st_join(oc_sf)
```

Determine the number of cleanup sites (points) within each census tract (polygon).
```{r}
# count number of points per polygon
cleanup_counts <- oc_join %>% 
  count(NAME)
```

Summary Stats:
```{r}
mean(cleanup_counts$n)
median(cleanup_counts$n)
max(cleanup_counts$n)
min(cleanup_counts$n)
```

```{r}
unique(cleanup_counts$n)
```

```{r}
cleanup_counts1 <- cleanup_counts %>% 
  filter(n == "1")

count(cleanup_counts1) # 8,176 of 9,129 census tracts have only 1 cleanup site
```


# Sept. 2020 "Cleanup Month" 

## Buffering Cleanup locations

```{r}
# filter for Sept. 2020 cleanup month data
sept2020 <- oc_sf %>% 
  filter(cleanup_date >= '2020-09-01' & cleanup_date <= '2020-09-30')
```

```{r}
# check crs
st_crs(sept2020) #NAD83, ESPG 4269
```

```{r}
# convert data frame to sf object
sept2020sites <- st_as_sf(sept2020, coords = "gps")

# convert sp to sf
#tracts <- st_as_sf(ca_acs)

# add cleanup site coordinate system to tracts
#st_crs(sept2020sites) = st_crs(ca_acs)
```

Buffer each clean-up site/point by 500 m
```{r}
# before buffering points, transform geographic CRS to a projected CRS (metric coordinate system)
sept2020sites_buffer = st_transform(sept2020sites, "+proj=longlat +datum=NAD83 +units=m") %>%
  # create buffer around each cleanup site by 500 m
  st_buffer(dist = units::set_units(500, m))

#tracts_km = st_transform(tracts, "+proj=longlat +datum=NAD83 +units=km")
```

## Map Buffered Clean-up Sites

```{r}
# set tmap viewing mode to interactive
tmap_mode("view")

# make a tmap of buffered points/clean-up sites
tm_shape(sept2020sites_buffer) +
  tm_dots(col = "sienna2", alpha = 0.3)
```

```{r}
# view buffered sites for LA county only
la_county <- sept2020sites_buffer %>% 
  filter(zone == "Los Angeles County, CA, USA")

#la_tracts <- ca_acs %>% 
#  filter(str_detect(NAME, 'Los Angeles'))

# static map of LA County buffered points
ggplot() +
  geom_sf(data = la_county) 

# interactive tmap 
tm_shape(la_county) +
  tm_dots(col = "sienna2", alpha = 0.4)
```


## How many buffered point/clean-up site intersections?

For CA:
```{r}
# create a column containing a list of buffered point intersection(s) for each site 
sept2020sites_buffer$intersections <- st_intersects(sept2020sites_buffer)

# count number of times each buffered site overlaps/intersects with another buffered site
# i.e. count length of list minus 1, since it includes itself
sept2020sites_buffer$n_intersections <- (lengths(sept2020sites_buffer$intersections) - 1)
```

```{r}
# how many buffered clean-up site DO NOT overlap?
int_count <- sept2020sites_buffer %>% 
  filter(n_intersections == "0")

count(int_count) # only 568 of 3,829 buffered clean-up sites DO NOT OVERLAP (with a 500 m buffer)
```

```{r}
# Summary Stats:
mean(sept2020sites_buffer$n_intersections)
median(sept2020sites_buffer$n_intersections)
max(sept2020sites_buffer$n_intersections)
min(sept2020sites_buffer$n_intersections)
```




For LA County only: (smaller dataset purposes)
```{r}
# compute area of intersection
la_county$area = st_area(la_county)

# create a column containing a list of buffered point intersection(s) for each site 
la_county$intersections <- st_intersects(la_county)
la_county$intersections

# count number of times each buffered site overlaps/intersects with another buffered site
la_county$n_intersections <- (lengths(la_county$intersections) - 1)
la_county$n_intersections


# ..... maybe make some sort of for loop... when point count is greater than 0, select site with earliest 'cleanup_date' and remove all other overlapping sites???
```








```{r}
# for loop
for (i in 1:pt_count(la_county)){
  
  # if a buffered point has more than 1 intersection
  if (la_county$pt_count > 1){
    
  } else {
    
  }
}
```










--------------

CA Shoreline shapefile (not needed?)
```{#r}
# read in CA shoreline 
ca_shoreline <- read_sf(here("data","CA_Shoreline","cstlne_simple.shp")) %>% 
  clean_names()
```

```{#r}
ggplot() +
  geom_sf(data = ca_shoreline)
```

