---
title: "process_spatial03 - isolate 2020 data, remove cleanups within 500m buffer"
author: "Sydney Rilum"
date: "2023-07-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# read in packages
library(tidyverse)
library(here)
library(janitor)

library(tidycensus)   # for census data
#census_api_key("9e2f392f2e528a2aa2f46238ae0a192d45b96ad8", install = TRUE) # only need to run this code once
library(sf)
library(purrr)
library(stringr)
library(tmap)
library(sp)       # for data transformation, CRS projection
library(rgeos)    # for buffering points
```

# Read in processed spatial data
```{r}
spdata_main <- read_csv(here("data", "processed", "spatialdata_01_main.csv"))
```

# Map clean-up sites

```{r}
# create a new sf dataframe that uses GPS column to make geometry for mapping purposes
spdata_sf <- spdata_main %>% 
  # split up gps column into lat and long columns
  separate(gps, into = c("latitude", "longitude"), sep = ", ", remove = FALSE, extra = "merge") %>%
  # use lat and long coordinates to create a point geometry column
  st_as_sf(., coords = c("longitude","latitude"))

head(spdata_sf) # check geometry type (hidden)
```

```{r}
# set CRS
st_crs(spdata_sf) <- 4269 # EPSG for NAD83

# check CRS
st_crs(spdata_sf)
```

```{r}
# map cleanup sites
ggplot() +
  geom_sf(data = spdata_sf) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Trash Cleanup Sites")
```

# Read in Census Tract Data

Use `tidycensus` package to obtain census track data of California.
```{r}
# view 2020 census data variables
#var <- load_variables(2020, "pl", cache = TRUE)
#view(var)

# view 2021 ACS data variables (NOTE: 2022 ACS data not available yet...)
var_acs <- load_variables(2021, "acs5", cache = TRUE)
view(var_acs)
```

```{r}
# to cache shapefiles for use in future sessions
options(tigris_use_cache = TRUE)

# obtain data and feature geometry from the 2020 census (from tidycensus package)
#ca_census <- get_decennial(geography = "tract",
#                           variables = "H1_001N",
#                           year = 2020,
#                           state = "CA",
#                           geometry = TRUE)

# obtain CA census track data and feature geometry from the 2017-2021 5 year ACS (American Community Survey)
ca_acs <- get_acs(geography = "tract",
                  survey = "acs5",
                  variables = "B01003_001", # total population (estimate and MOE/margin-of-error)
                  year = 2021,
                  state = "CA",
                  geometry = TRUE) # obtain geometry
```

```{r}
# map census tracts
ggplot() +
  geom_sf(data = ca_acs) +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{#r}
# map census tract population density
ggplot() +
  geom_sf(data = ca_acs, aes(fill = estimate), color = "black") +
  scale_fill_gradientn(colors = c("lightgray","orange","red")) +
  theme_minimal() +
  labs(fill = "ACS Census Tract Population Estimates",
       x = "Longitude",
       y = "Latitude", 
       title = "California 2017-2021 ACS Census Tracts")
```

```{r}
# check CRS (Coordinate Reference System) match
st_crs(ca_acs) # NAD83
st_crs(spdata_sf)
```


# Spatial Join

```{r}
# map trash cleanup sites over census tract outlines
ggplot() +
  geom_sf(data = ca_acs) +
  geom_sf(data = spdata_sf, color = "blue") +
  theme_minimal() +
  labs(x = "Longitude",
       y = "Latitude", 
       title = "Coastal Cleanup Sites")
```

Conduct a spatial join of ACS Census tracts (polygons) and Trash Cleanup sites (points).
```{r}
# spatial join
spjoin <- ca_acs %>% 
  st_join(spdata_sf)
```

Determine the number of cleanup sites (points) within each census tract (polygon).
```{r}
# count number of points per polygon
cleanup_counts <- spjoin %>% 
  count(NAME)
```

Summary Stats:
```{r}
mean(cleanup_counts$n)
median(cleanup_counts$n)
max(cleanup_counts$n)
min(cleanup_counts$n)
```

```{r}
unique(cleanup_counts$n)
```

```{r}
cleanup_counts1 <- cleanup_counts %>% 
  filter(n == "1")

count(cleanup_counts1) # 8,289 of 9,129 census tracts have only 1 cleanup site
```


# Sept. 2020 "Cleanup Month" 

## Include data from year 2020 only

```{r}
# filter for Sept. 2020 cleanup month data
spdata_2020 <- spdata_sf %>% 
  filter(cleanup_date >= '2020-09-01' & cleanup_date <= '2020-09-30')
```

```{r}
# check crs
st_crs(spdata_2020) #NAD83, ESPG 4269
```

```{r}
# convert data frame to sf object
spdata_2020sf <- st_as_sf(spdata_2020, coords = "gps")

# convert sp to sf
#tracts <- st_as_sf(ca_acs)

# add cleanup site coordinate system to tracts
#st_crs(spdata_2020) = st_crs(ca_acs)
```

## Buffer 2020 Cleanup locations

Buffer each clean-up site/point by 500 m
```{r}
# before buffering points, transform geographic CRS to a projected CRS (metric coordinate system)
spdata_2020buffer = st_transform(spdata_2020sf, "+proj=longlat +datum=NAD83 +units=m") %>%
  # create buffer around each cleanup site by 500 m
  st_buffer(dist = units::set_units(500, m))

#tracts_km = st_transform(tracts, "+proj=longlat +datum=NAD83 +units=km")
```

## Map Buffered Clean-up Sites

```{r}
# set tmap viewing mode to interactive
tmap_mode("view")

# make a tmap of buffered points/clean-up sites
tm_shape(spdata_2020buffer) +
  tm_dots(col = "sienna2", alpha = 0.3)
```

Exploratory plot for LA County
```{r}
# view buffered sites for LA county only
la_county <- spdata_2020buffer %>% 
  filter(zone == "Los Angeles County, CA, USA")

#la_tracts <- ca_acs %>% 
#  filter(str_detect(NAME, 'Los Angeles'))

# static map of LA County buffered points
ggplot() +
  geom_sf(data = la_county) 

# interactive tmap 
tm_shape(la_county) +
  tm_dots(col = "sienna2", alpha = 0.4)
```


## How many buffered point/clean-up site intersections?

For CA:
```{r}
# create a column containing a list of buffered point intersection(s) for each site 
spdata_2020buffer$intersections <- st_intersects(spdata_2020buffer)

# count number of times each buffered site overlaps/intersects with another buffered site
# i.e. count length of list minus 1, since it includes itself
spdata_2020buffer$n_intersections <- (lengths(spdata_2020buffer$intersections) - 1)
```

```{r}
# how many buffered clean-up site DO NOT overlap?
int_count <- spdata_2020buffer %>% 
  filter(n_intersections == "0")

count(int_count) # only 486 of 2,588 buffered clean-up sites DO NOT OVERLAP (with a 500 m buffer)
```

```{r}
# Summary Stats:
mean(spdata_2020buffer$n_intersections)
median(spdata_2020buffer$n_intersections)
max(spdata_2020buffer$n_intersections)
min(spdata_2020buffer$n_intersections)
```




For LA County only: (smaller dataset purposes)
```{r}
# compute area of intersection
la_county$area = st_area(la_county)

# create a column containing a list of buffered point intersection(s) for each site 
la_county$intersections <- st_intersects(la_county)
la_county$intersections

# count number of times each buffered site overlaps/intersects with another buffered site
la_county$n_intersections <- (lengths(la_county$intersections) - 1)
la_county$n_intersections


# ..... maybe make some sort of for loop... when point count is greater than 0, select site with earliest 'cleanup_date' and remove all other overlapping sites???
```





# Remove cleanups within 500 m buffer


Will try ChatGPT!!!


```{r}
# for loop
for (i in 1:pt_count(la_county)){
  
  # if a buffered point has more than 1 intersection
  if (la_county$pt_count > 1){
    
  } else {
    
  }
}
```







# Save dataframe as spatialdata_03_2020_500m.csv
```{r}
write_csv(spdata_500mbuffer, here("data", "processed", "spatialdata_03_2020_500m.csv"))
```
